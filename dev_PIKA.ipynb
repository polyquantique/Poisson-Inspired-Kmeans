{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "from scipy.special import loggamma\n",
    "import matplotlib.pyplot as plt \n",
    "from typing import Union\n",
    "import polars as pl\n",
    "\n",
    "PATH_DATA = '/home/nicolasdc/Documents/Datasets/Photon-Number-Classification/NRC CSV/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_ = ['traces_attn_31dB.csv']\n",
    "data = []\n",
    "for i in files_:\n",
    "    data_ =  pl.read_csv(rf'{PATH_DATA}/{i}', has_header=False, separator=\",\").to_numpy()\n",
    "    mean_it = data_[:,:10].mean()\n",
    "    data.append((data_[:,::3] - mean_it))\n",
    "\n",
    "data = np.concatenate(data, axis=0)\n",
    "data_train = data[::2]\n",
    "data_test = data[1::2]\n",
    "mean_ = data_train.mean()\n",
    "std_ = data_train.std()\n",
    "data_train  = (data_train - mean_) / std_\n",
    "data_test  = (data_test - mean_) / std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 200)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialSetup():\n",
    "    pass\n",
    "\n",
    "def readAndFilter():\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleted Functions and Variables\n",
    "\n",
    "### Variables\n",
    "\n",
    "- `iNPhoton` : Dataset creation is not included in the processing \n",
    "- `iDataSet` : Dataset creation is not included in the processing \n",
    "- `nTimes` : Dataset creation is not included in the processing \n",
    "- `mSample` : Visual \n",
    "- `nDatUse` : Dataset creation is not included in the processing \n",
    "- `backgroundReject` : Dataset creation is not included in the processing \n",
    "- `peakValCut` : Dataset creation is not included in the processing \n",
    "- `peakPosCut` : Dataset creation is not included in the processing \n",
    "- `peakNumCut` : Dataset creation is not included in the processing \n",
    "- `coolConst` :\n",
    "- `tAnneal` :\n",
    "- `probDistName` : Visual\n",
    "- `binFract` : Visual\n",
    "- `outputImageSize` : Visual\n",
    "- `partialFilePath` : Dataset creation is not included in the processing \n",
    "- `fileExt` : Dataset creation is not included in the processing \n",
    "- `nSamplePerTrace` : Dataset creation is not included in the processing \n",
    "- `nTracePerFile` : Dataset creation is not included in the processing \n",
    "- `useInputFile` : Dataset creation is not included in the processing \n",
    "- `pikaInput` : Dataset creation is not included in the processing \n",
    "\n",
    "### Functions\n",
    "\n",
    "- `initialSetup` : Visual\n",
    "- `readAndFilterData` : Dataset creation is not included in the processing \n",
    "- `optionallyRejectBackgroundTraces` : Dataset creation is not included in the processing\n",
    "- `graphGlobalMeanTrace` : Visual \n",
    "- `graphSampleMiddleClusters` : Visual\n",
    "- `graphClusterMeans` : Visual\n",
    "- `graphSampleMiddleClusters` : Visual\n",
    "- `graphClusterMean` : Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class PIKA:\n",
    "\n",
    "    def __init__(self,\n",
    "                nCool : int = 10,\n",
    "                nGreedy : int = 10,\n",
    "                nSigma : float = 4.,\n",
    "                verbose : bool = False\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        iNPhoton—the index of the dataset to read (assuming that multiple datasets are stored in the same location)\n",
    "        iDataSet—a list of indices of parts of the dataset to read (assuming that datasets are split over multiple files)\n",
    "        nTime—the number of time points to keep after filtration\n",
    "        mSample—the maximum number of traces to randomly sample when outputting samples\n",
    "        nDatUse—the maximum number of traces to read in (if this is equal to 0, we use all of the traces)\n",
    "        backgroundReject—a Boolean, true if PIKA should reject response waveforms with background radiation\n",
    "        peakValCut—the voltage cutoff for background rejection\n",
    "        peakPosCut—the peak location cutoff for background rejection\n",
    "        peakNumCut—the cutoff for number of peaks for background rejection\n",
    "        nCool—the number of optimizing rounds to perform\n",
    "        nGreedy—the number of rounds that should be run with the greedy algorithm\n",
    "        coolConst—the cooling constant for simulated annealing\n",
    "        tAnneal—the starting annealing temperature\n",
    "        probDistName—the name of the probability distribution\n",
    "        nSigma—the number of standard deviations from the mean of the probability distribution to generate\n",
    "        binFract—the histogram bin size (fraction of a photon number)\n",
    "        outputImageSize—the default graphics size for output\n",
    "        nPhotonAvgList—a list of (close together) mean photon numbers with which to run PIKA on the dataset\n",
    "        partialFilePath—the directory and the beginning of the name of the data files, without the iNPhoton or iDataSet markers\n",
    "        fileExt—the file extension of the data files (without the iDataSet marker)\n",
    "        nSamplePerTrace—the number of samples per trace\n",
    "        nTracePerFile—the number of traces per file\n",
    "        useInputFile—a Boolean, true if there is another options file to read in and override settings from the form\n",
    "        pikaInput—the path to the options override file\n",
    "        \"\"\"\n",
    "        \n",
    "        self.m = None \n",
    "        self.n = None\n",
    "        self.s = None\n",
    "\n",
    "        # Visual\n",
    "        self.mSample = 1000\n",
    "        self.binFract = 500\n",
    "        self.dpi = 100\n",
    "        self.outputImageSize = (6,3)\n",
    "        self.styleName = 'seaborn-v0_8'\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Optimization\n",
    "        self.nCool = nCool\n",
    "        self.nGreedy = nGreedy\n",
    "\n",
    "        # Model\n",
    "        self.nSigma = nSigma\n",
    "        # self.nPhotonAvgList = nPhotonAvgList\n",
    "\n",
    "\n",
    "    def findDotProductEffectivePhotonNumbers(self,\n",
    "                                        data : np.array, \n",
    "                                        nPhotonAvg : float,\n",
    "                                        meanTrace : np.array,\n",
    "                                        verbose : bool = False\n",
    "                                        ):\n",
    "        \"\"\"\n",
    "        Compute n_eff for every trace.\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        nPhotonEff = nPhotonAvg * np.dot(data, meanTrace) / np.linalg.norm(meanTrace)\n",
    "\n",
    "        if verbose:\n",
    "            with plt.style.context(self.styleName):\n",
    "                plt.figure(figsize = self.outputImageSize, dpi=self.dpi)\n",
    "                plt.hist(nPhotonEff, alpha=0.8, bins=self.binFract)\n",
    "                plt.ylabel('Voltage (a.u.)')\n",
    "                plt.xlabel('Time (a.u.)')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "        return nPhotonEff\n",
    "    \n",
    "    def groupProb(self,\n",
    "                  data : np.array,\n",
    "                  nPhoton : np.array,\n",
    "                  probCum : np.array,\n",
    "                  nPhotonEff : np.array,\n",
    "                  ):\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        - Can't we just split instead of finding start and stop?\n",
    "            - np.split with cmf\n",
    "        \"\"\"\n",
    "        # Shape (n,)\n",
    "        nPhotonEffSortIndex = np.argsort(nPhotonEff)\n",
    "        # Shape (n_cluster, inconsistent)\n",
    "        iObsOfClust = np.array_split(\n",
    "                                    nPhotonEffSortIndex, \n",
    "                                    np.unique(np.round(self.n * probCum).astype(int))[:-1]\n",
    "                                    )\n",
    "        # Shape (n_cluster, s)\n",
    "        clustMeanTrace = np.array([np.mean(data[nClust], axis=0) for nClust in iObsOfClust])\n",
    "        # Shape (n_cluster,)\n",
    "        nPhotonUse = nPhoton[:len(iObsOfClust)] #TODO make this more robust \n",
    "            \n",
    "        return nPhotonUse, iObsOfClust, clustMeanTrace\n",
    "\n",
    "    \n",
    "    def createInitialClusters(self, \n",
    "                            data : np.array,\n",
    "                            nPhotonEff : np.array,\n",
    "                            nPhotonAvg : float\n",
    "                            ):\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        - Is prob useful?\n",
    "            So far only use cdf.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        \n",
    "        # Poisson function in article\n",
    "        ##  Probability distribution based on Confidence interval\n",
    "        sigma = np.sqrt(nPhotonAvg)\n",
    "        nPhoton = np.arange(\n",
    "                            start = max(int(np.floor(nPhotonAvg - sigma * self.nSigma)), 0), \n",
    "                            stop = int(np.ceil(nPhotonAvg + sigma * self.nSigma)) + 1,\n",
    "                            step = 1\n",
    "                            )\n",
    "        prob = poisson.pmf(nPhoton, nPhotonAvg)\n",
    "        prob = prob / np.sum(prob)\n",
    "\n",
    "        probCum = poisson.cdf(nPhoton, nPhotonAvg)\n",
    "        \n",
    "        nPhotonOfClust, iObsOfClust, clustMeanTrace = self.groupProb(\n",
    "                                                                data = data,\n",
    "                                                                nPhoton = nPhoton,\n",
    "                                                                probCum = probCum,\n",
    "                                                                nPhotonEff = nPhotonEff\n",
    "                                                                )\n",
    "\n",
    "        return prob, nPhotonOfClust, iObsOfClust, clustMeanTrace\n",
    "\n",
    "    def comboLogLikelihood(self, m):\n",
    "        \"\"\"\n",
    "        L_c\n",
    "\n",
    "        m : np.array\n",
    "            Array containing the size of the clusters.\n",
    "        \"\"\"\n",
    "        return loggamma(self.m + 1) - np.sum(loggamma(m + 1))\n",
    "\n",
    "    def poissonLogLikehood(self, m, mu):\n",
    "        \"\"\"\n",
    "        L_p\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        return - mu * self.m + nList\n",
    "    \n",
    "    def probComboLogLikelihood(self):\n",
    "        pass\n",
    "\n",
    "    def findInitialObjectiveFunction(self,\n",
    "                                     data : np.array,\n",
    "                                     iObsOfClust : list,\n",
    "                                     \n",
    "                                     clustMeanTrace : np.array):\n",
    "        \"\"\"\n",
    "        TODO : Avoid loop \n",
    "        \"\"\"\n",
    "        sqDevOfClust = []\n",
    "        nOfClust = []\n",
    "\n",
    "        for iClust, nClust in enumerate(iObsOfClust):\n",
    "            size_ = nClust.size\n",
    "            sqDevOfClust.append(np.sum((data[nClust] - clustMeanTrace[iClust])**2)/size_)\n",
    "            nOfClust.append(size_)\n",
    "\n",
    "        sqDevOfClust = np.array(sqDevOfClust)\n",
    "        nOfClust = np.array(nOfClust)        \n",
    "      \n",
    "        logLikeProb = \n",
    "\n",
    "        sigmaObjFtn = np.sqrt()\n",
    "        \n",
    "        return sqDevOfClust / (2 * sigmaObjFtn**2) - logLikeProb\n",
    "\n",
    "    def fit(self, \n",
    "            data : np.array, \n",
    "            nPhotonAvgList : Union[np.array, list]\n",
    "            ):\n",
    "        \"\"\"\n",
    "        data : np.array\n",
    "            Shape (m,n,s)\n",
    "                - `m` : Number of coherent sources in the dataset (equal to len(nPhotonAvgList)).\n",
    "                - `n` : Number of samples in the set associated with a given mean phton number.\n",
    "                - `s` : Size of each trace (number of time steps).\n",
    "        \"\"\"\n",
    "\n",
    "        assert data.shape[0] == len(nPhotonAvgList), \\\n",
    "            'The number of sources in `data` should be equal to the number of means in `nPhotonAvgList`'\n",
    "        \n",
    "        # Shape (m,n,s)\n",
    "        self.m, self.n, self.s = data.shape\n",
    "        # Shape (s,)\n",
    "        meanTrace = np.mean(data[0], axis = 0)\n",
    "        # (n,)\n",
    "        nPhotonEff = self.findDotProductEffectivePhotonNumbers(\n",
    "            data = data[0],\n",
    "            nPhotonAvg = nPhotonAvgList[0],\n",
    "            meanTrace = meanTrace\n",
    "        )\n",
    "\n",
    "        for iNPhotonAvgList, nPhotonAvg in enumerate(nPhotonAvgList):\n",
    "            \n",
    "\n",
    "            prob, nPhotonOfClust, iObsOfClust, clustMeanTrace = self.createInitialClusters(\n",
    "                data = data[iNPhotonAvgList],\n",
    "                nPhotonEff = nPhotonEff,\n",
    "                nPhotonAvg = nPhotonAvg\n",
    "            )\n",
    "            \n",
    "            objFtn = self.findInitialObjectiveFunction(\n",
    "                data = data[iNPhotonAvgList],\n",
    "                iObsOfClust = iObsOfClust,\n",
    "                clustMeanTrace = clustMeanTrace\n",
    "            )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk = PIKA(\n",
    "        nCool = 10,\n",
    "        nGreedy = 10,\n",
    "        nSigma = 4.\n",
    "        )\n",
    "\n",
    "pk.fit(\n",
    "        data =data_train[np.newaxis, :100, :], \n",
    "        nPhotonAvgList = [3],\n",
    "      )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
